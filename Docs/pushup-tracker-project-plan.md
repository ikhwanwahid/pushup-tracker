# CS604 Push-Up Tracker — Project Plan

## 1. Problem Statement

Automated exercise form analysis requires accurate, real-time human pose estimation. No single pose estimation model dominates across all metrics — speed, accuracy, and robustness vary significantly depending on the model architecture and the conditions of the input video. This project builds a push-up tracking system that:

1. **Compares multiple pose estimation models** (YOLO11s-pose, MediaPipe Pose, MoveNet Lightning, MoveNet Thunder) on the same dataset using a unified evaluation framework.
2. **Counts push-up repetitions** using both a rule-based baseline (finite state machine) and a learned approach (bidirectional LSTM), then compares the two.
3. **Assesses push-up form quality** by scoring back alignment, depth, and arm extension per repetition.

## 2. Dataset

- **Source**: Kaggle push-up video dataset
- **Size**: 100 videos — 50 correct form, 50 incorrect form
- **Labels provided**: Binary (correct/incorrect) + pre-extracted keypoint arrays (`.npy`)
- **Labels generated**: Frame-level phase labels (UP, GOING_DOWN, DOWN, GOING_UP) auto-generated by the state machine for LSTM training (semi-supervised)

Location: `data/raw/kaggle_pushups/`

## 3. Multi-Model Comparison

### 3.1 Models Under Evaluation

| Model | Architecture | Native Keypoints | Input Size | Runtime |
|-------|-------------|-----------------|-----------|---------|
| YOLO11s-pose | CNN + pose head | 17 (COCO) | Variable | PyTorch |
| MediaPipe Pose | BlazePose (3 complexity levels) | 33 | Variable | MediaPipe |
| MoveNet Lightning | MobileNetV2 backbone | 17 (COCO) | 192x192 | TFLite |
| MoveNet Thunder | Higher-capacity backbone | 17 (COCO) | 256x256 | TFLite |

### 3.2 Unified 12-Joint Format

To enable fair, model-agnostic comparison, all models map their native keypoints to a common 12-joint schema covering the joints relevant to push-up biomechanics:

| Index | Joint | Index | Joint |
|-------|-------|-------|-------|
| 0 | Left Shoulder | 6 | Left Hip |
| 1 | Right Shoulder | 7 | Right Hip |
| 2 | Left Elbow | 8 | Left Knee |
| 3 | Right Elbow | 9 | Right Knee |
| 4 | Left Wrist | 10 | Left Ankle |
| 5 | Right Wrist | 11 | Right Ankle |

Each keypoint stores `[x_pixel, y_pixel, confidence]`. Native keypoints are also preserved for benchmark fairness.

### 3.3 Benchmark Metrics

- **Detection rate**: % of frames where a person is detected
- **Per-joint confidence**: Mean confidence per unified joint across all frames
- **Inference latency**: Mean, median, P95, min/max (ms per frame)
- **Downstream accuracy**: How each model's keypoints affect rep counting and quality scoring

## 4. Repetition Counting

### 4.1 Baseline — Rule-Based State Machine

A 4-phase finite state machine driven by elbow angle:

```
UP  -->  GOING_DOWN  -->  DOWN  -->  GOING_UP  -->  UP (count += 1)
         (angle < 160)    (angle < 90)  (angle > 90)   (angle > 160)
```

Configurable thresholds in `configs/quality_thresholds.yaml`.

### 4.2 Learned — Bidirectional LSTM

- **Input**: Sliding windows of angle features (elbow, back alignment, hip, knee) — shape `(batch, seq_len, 4)`
- **Output**: Frame-level phase probabilities — shape `(batch, seq_len, 4)`
- **Rep counting**: Count GOING_UP -> UP transitions in predicted sequence
- **Training labels**: Auto-generated by the state machine (semi-supervised — no manual frame-level annotation)
- **Validation**: Leave-one-video-out cross-validation using scikit-learn's `LeaveOneGroupOut`

### 4.3 Comparison

Both methods are evaluated on the same videos. Metrics:
- Rep count accuracy (predicted vs ground truth)
- Phase classification accuracy (LSTM only)
- Per-video error distribution

## 5. Quality Assessment

Each completed rep is scored on three criteria (0-100 scale):

| Criterion | What it measures | Ideal |
|-----------|-----------------|-------|
| Back Alignment | Shoulder-hip-ankle angle throughout | ~180 deg (straight line) |
| Depth | Minimum elbow angle at bottom | <= 90 deg |
| Extension | Maximum elbow angle at top | >= 170 deg |

Composite score = weighted average (back 40%, depth 35%, extension 25%). Text feedback is generated for each criterion that falls below threshold.

## 6. Project Timeline

| Phase | Week | Focus | Key Deliverables |
|-------|------|-------|-----------------|
| 1 | 1 | Foundation — unified interface, all 4 model wrappers, unit tests | Working estimators, `predict_frame()` on all models |
| 2 | 2 | Multi-model benchmark — run all models on full dataset | Benchmark CSV, detection/latency comparison notebook |
| 3 | 2-3 | Feature engineering + rule-based counting baseline | Angle extraction pipeline, state machine counter, baseline accuracy |
| 4 | 3-4 | LSTM rep counter — train, cross-validate, compare vs baseline | Trained model, accuracy comparison |
| 5 | 4-5 | Quality assessment + validation video recording | Scoring system, feedback generation, self-recorded test videos |
| 6 | 5-6 | Full pipeline evaluation + live webcam demo | End-to-end results, working demo |
| 7 | 6-7 | Report writing + presentation | Final report, figures, slides |

### Current Status (Phase 1 complete)

- All 4 model wrappers implemented and importable
- Unified 12-joint mapping verified for YOLO, MediaPipe, MoveNet
- Benchmark harness ready to run
- Feature engineering, counting, and quality modules implemented
- 23 unit tests passing
- 8 notebook stubs created for analysis workflow

## 7. Notebook Workflow

| Notebook | Phase | Purpose |
|----------|-------|---------|
| `01_data_exploration` | 1 | Video metadata, label distribution, sample frames |
| `02_model_comparison` | 2 | Side-by-side skeleton visualizations |
| `03_benchmark_analysis` | 2 | Detection rate, latency, confidence analysis |
| `04_feature_engineering` | 3 | Angle computation, normalization, temporal features |
| `05_rep_counting_baseline` | 3 | State machine evaluation |
| `06_rep_counting_lstm` | 4 | LSTM training, cross-validation, baseline comparison |
| `07_quality_assessment` | 5 | Scoring and feedback on correct vs incorrect videos |
| `08_final_evaluation` | 6 | End-to-end pipeline results, summary figures |

## 8. Project Structure

```
pushup-tracker/
├── configs/
│   └── quality_thresholds.yaml        # Scoring + counting thresholds
├── data/
│   ├── raw/kaggle_pushups/            # 100 videos + binary labels
│   ├── processed/
│   │   ├── keypoints/{yolo,mediapipe,movenet_lightning,movenet_thunder}/
│   │   ├── features/
│   │   └── benchmark/
│   └── annotations/
├── src/
│   ├── pose_estimation/               # Base ABC, schema, 3 estimator wrappers, viz
│   ├── features/                      # Angles, normalization, temporal
│   ├── counting/                      # State machine, LSTM, dataset, training
│   ├── quality/                       # Scoring, feedback
│   ├── benchmark/                     # Multi-model benchmark runner
│   └── demo/                          # Live webcam demo
├── notebooks/                         # 01-08 analysis notebooks
├── models/                            # Saved weights (gitignored)
├── outputs/{figures,videos,results}/  # Generated outputs
├── tests/                             # Unit tests (23 tests)
├── Docs/                              # This document
└── pyproject.toml                     # Dependencies + config
```

## 9. Key Design Decisions

| Decision | Rationale |
|----------|-----------|
| Unified 12-joint format | Lowest common denominator across all models for push-up analysis; enables model-agnostic features |
| Semi-supervised LSTM labels | State machine auto-labels 100 videos frame-by-frame — no manual annotation needed |
| onnxruntime over tflite-runtime | tflite-runtime unavailable on macOS ARM; onnxruntime is cross-platform |
| 3 model families (not more) | YOLO + MediaPipe + MoveNet covers CNN, BlazePose, and MobileNet architectures without the fragility of mmpose/RTMPose |
| Leave-one-video-out CV | Prevents data leakage from sliding window overlap within the same video |
| Bidirectional LSTM | Looks at both past and future frames for more accurate phase classification |
