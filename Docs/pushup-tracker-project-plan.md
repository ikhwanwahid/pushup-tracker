# CS604 Push-Up Tracker — Project Plan

## 1. Problem Statement

Automated exercise form analysis requires accurate, real-time human pose estimation. No single pose estimation model dominates across all metrics — speed, accuracy, and robustness vary significantly depending on the model architecture and the conditions of the input video. This project builds a push-up tracking system that:

1. **Compares multiple pose estimation models** (YOLO11s-pose, MediaPipe Pose, MoveNet Lightning, MoveNet Thunder) on the same dataset using a unified evaluation framework.
2. **Classifies push-up form** using a comparison of two CV paradigms: a pretrained 3D CNN (R3D-18) operating on raw video frames vs a Spatial-Temporal GCN operating on skeleton sequences, evaluated against a logistic regression baseline.
3. **Counts push-up repetitions** using a rule-based state machine.
4. **Assesses push-up form quality** by scoring back alignment, depth, and arm extension per repetition.

## 2. Dataset

- **Source**: Kaggle push-up video dataset
- **Size**: 100 videos — 50 correct form, 50 incorrect form
- **Labels provided**: Binary (correct/incorrect) + pre-extracted keypoint arrays (`.npy`)
- **Labels generated**: Frame-level phase labels (UP, GOING_DOWN, DOWN, GOING_UP) auto-generated by the state machine for rep counting

Location: `data/raw/kaggle_pushups/`

## 3. Multi-Model Comparison

### 3.1 Models Under Evaluation

| Model | Architecture | Native Keypoints | Input Size | Runtime |
|-------|-------------|-----------------|-----------|---------|
| YOLO11s-pose | CNN + pose head | 17 (COCO) | Variable | PyTorch |
| MediaPipe Pose | BlazePose (3 complexity levels) | 33 | Variable | MediaPipe |
| MoveNet Lightning | MobileNetV2 backbone | 17 (COCO) | 192x192 | TFLite |
| MoveNet Thunder | Higher-capacity backbone | 17 (COCO) | 256x256 | TFLite |

### 3.2 Unified 12-Joint Format

To enable fair, model-agnostic comparison, all models map their native keypoints to a common 12-joint schema covering the joints relevant to push-up biomechanics:

| Index | Joint | Index | Joint |
|-------|-------|-------|-------|
| 0 | Left Shoulder | 6 | Left Hip |
| 1 | Right Shoulder | 7 | Right Hip |
| 2 | Left Elbow | 8 | Left Knee |
| 3 | Right Elbow | 9 | Right Knee |
| 4 | Left Wrist | 10 | Left Ankle |
| 5 | Right Wrist | 11 | Right Ankle |

Each keypoint stores `[x_pixel, y_pixel, confidence]`. Native keypoints are also preserved for benchmark fairness.

### 3.3 Benchmark Metrics

- **Detection rate**: % of frames where a person is detected
- **Per-joint confidence**: Mean confidence per unified joint across all frames
- **Inference latency**: Mean, median, P95, min/max (ms per frame)
- **Downstream accuracy**: How each model's keypoints affect rep counting and quality scoring

## 4. Repetition Counting

### 4.1 Baseline — Rule-Based State Machine

A 4-phase finite state machine driven by elbow angle:

```
UP  -->  GOING_DOWN  -->  DOWN  -->  GOING_UP  -->  UP (count += 1)
         (angle < 160)    (angle < 90)  (angle > 90)   (angle > 160)
```

Configurable thresholds in `configs/quality_thresholds.yaml`.

## 4.5 Form Classification

### Baseline — Logistic Regression
- 16 per-video angle statistics (mean/min/max/range x 4 angles)
- StandardScaler + LogisticRegression pipeline via scikit-learn

### 3D CNN — R3D-18
- Pretrained on Kinetics-400 (torchvision), frozen backbone
- Fine-tune final FC layer (512 -> 2) for correct/incorrect
- Input: 16 uniformly sampled video frames, 112x112

### ST-GCN — Spatial-Temporal Graph Convolutional Network
- Custom implementation using standard PyTorch (no torch_geometric)
- Graph structure from unified 12-joint skeleton (12 nodes, 12 edges)
- 3 ST-GCN blocks (2->64->64->128), global avg pool -> FC(128, 2)
- Input: Torso-normalized (x,y) keypoint sequences, padded to 150 frames

### Evaluation
- 5-fold stratified CV with identical splits across all 3 methods
- Metrics: accuracy, F1, confusion matrix, per-video agreement

## 5. Quality Assessment

Each completed rep is scored on three criteria (0-100 scale):

| Criterion | What it measures | Ideal |
|-----------|-----------------|-------|
| Back Alignment | Shoulder-hip-ankle angle throughout | ~180 deg (straight line) |
| Depth | Minimum elbow angle at bottom | <= 90 deg |
| Extension | Maximum elbow angle at top | >= 170 deg |

Composite score = weighted average (back 40%, depth 35%, extension 25%). Text feedback is generated for each criterion that falls below threshold.

## 6. Project Timeline

| Phase | Week | Focus | Key Deliverables |
|-------|------|-------|-----------------|
| 1 | 1 | Foundation — unified interface, all 4 model wrappers, unit tests | Working estimators, `predict_frame()` on all models |
| 2 | 2 | Multi-model benchmark — run all models on full dataset | Benchmark CSV, detection/latency comparison notebook |
| 3 | 2-3 | Feature engineering + counting baseline + form classification | Angle pipeline, state machine, Baseline vs R3D-18 vs ST-GCN comparison |
| 4 | 3-4 | Quality assessment + form classification evaluation | Quality scorer validation, form classification results |
| 5 | 4-5 | Quality assessment + validation video recording | Scoring system, feedback generation, self-recorded test videos |
| 6 | 5-6 | Full pipeline evaluation + live webcam demo | End-to-end results, working demo |
| 7 | 6-7 | Report writing + presentation | Final report, figures, slides |

### Current Status (Phases 1-3 complete)

- All 4 model wrappers implemented and importable
- Unified 12-joint mapping verified for YOLO, MediaPipe, MoveNet
- Benchmark harness ready to run
- Feature engineering, counting, and quality modules implemented
- Form classification module (ST-GCN, R3D-18, baseline) implemented
- 34 unit tests passing
- Notebooks 01-06 complete (data exploration through form classification)

## 7. Notebook Workflow

| Notebook | Phase | Purpose |
|----------|-------|---------|
| `01_data_exploration` | 1 | Video metadata, label distribution, sample frames |
| `02_model_comparison` | 2 | Side-by-side skeleton visualizations |
| `03_benchmark_analysis` | 2 | Detection rate, latency, confidence analysis |
| `04_feature_engineering` | 3 | Angle computation, normalization, temporal features |
| `05_rep_counting_baseline` | 3 | State machine evaluation |
| `06_form_classification` | 3-4 | Baseline vs R3D-18 vs ST-GCN comparison |
| `07_quality_assessment` | 5 | Scoring and feedback on correct vs incorrect videos |
| `08_final_evaluation` | 6 | End-to-end pipeline results, summary figures |

## 8. Project Structure

```
pushup-tracker/
├── configs/
│   └── quality_thresholds.yaml        # Scoring + counting thresholds
├── data/
│   ├── raw/kaggle_pushups/            # 100 videos + binary labels
│   ├── processed/
│   │   ├── keypoints/{yolo,mediapipe,movenet_lightning,movenet_thunder}/
│   │   ├── features/
│   │   └── benchmark/
│   └── annotations/
├── src/
│   ├── pose_estimation/               # Base ABC, schema, 3 estimator wrappers, viz
│   ├── features/                      # Angles, normalization, temporal
│   ├── classification/                # Form classification (ST-GCN, 3D CNN, baseline)
│   ├── counting/                      # State machine rep counter
│   ├── quality/                       # Scoring, feedback
│   ├── benchmark/                     # Multi-model benchmark runner
│   └── demo/                          # Live webcam demo
├── notebooks/                         # 01-08 analysis notebooks
├── models/                            # Saved weights (gitignored)
├── outputs/{figures,videos,results}/  # Generated outputs
├── tests/                             # Unit tests (34 tests)
├── Docs/                              # This document
└── pyproject.toml                     # Dependencies + config
```

## 9. Key Design Decisions

| Decision | Rationale |
|----------|-----------|
| Unified 12-joint format | Lowest common denominator across all models for push-up analysis; enables model-agnostic features |
| onnxruntime over tflite-runtime | tflite-runtime unavailable on macOS ARM; onnxruntime is cross-platform |
| 3 model families (not more) | YOLO + MediaPipe + MoveNet covers CNN, BlazePose, and MobileNet architectures without the fragility of mmpose/RTMPose |
| ST-GCN for form classification | Graph structure models spatial joint relationships explicitly; captures body topology |
| R3D-18 frozen backbone | Only 1,026 trainable params prevents overfitting on 100 videos |
| 5-fold stratified CV | More robust than LOGO-CV with 100 folds; stratification ensures balanced class splits |
